import cv2
import numpy as np
import time
from picamera2 import Picamera2
from collections import deque
import threading
import datetime
import os

# --- Importa√ß√µes da Hailo ---
from hailo_platform import (HEF, VDevice, HailoStreamInterface, ConfigureParams,
                            InputVStream, OutputVStream)

# --- Configura√ß√µes ---
HEF_PATH = 'yolov8s_pose.hef'
VIDEO_DURATION_SECONDS = 20
CAMERA_RESOLUTION = (640, 480) # Resolu√ß√£o para a c√¢mera e modelo
COOLDOWN_SECONDS = 30 # Tempo de espera em segundos ap√≥s salvar um v√≠deo

# --- Mapeamento de Pontos-Chave (Keypoints) do YOLOv8-Pose ---
# (Verifique se corresponde ao seu modelo, este √© o padr√£o COCO)
NOSE = 0
LEFT_SHOULDER = 5
RIGHT_SHOULDER = 6
LEFT_WRIST = 9
RIGHT_WRIST = 10

def setup_hailo_device(hef_path):
    """Inicializa o VDevice da Hailo e carrega o HEF."""
    print("üîé Inicializando dispositivo Hailo e carregando HEF...")
    devices = VDevice.scan()
    if not devices:
        raise RuntimeError("Nenhum dispositivo Hailo encontrado.")
    
    target = VDevice(devices[0])
    
    # Carrega o modelo compilado (HEF)
    hef = HEF(hef_path)
    
    # Configura os streams de entrada/sa√≠da
    configure_params = ConfigureParams.create_from_hef(hef, interface=HailoStreamInterface.PCIe)
    network_group = target.configure(hef, configure_params)[0]
    
    # Obtenha informa√ß√µes sobre input e output
    input_vstreams = InputVStream.make(network_group, transfer_mode=HailoStreamInterface.PCIe)
    output_vstreams = OutputVStream.make(network_group, transfer_mode=HailoStreamInterface.PCIe)
    
    print("‚úÖ Dispositivo Hailo pronto.")
    return target, hef, input_vstreams[0], output_vstreams[0]


def save_video_from_buffer(video_buffer, resolution, fps):
    """Salva os frames do buffer em um arquivo de v√≠deo."""
    if not video_buffer:
        print("‚ö†Ô∏è Buffer de v√≠deo vazio, nada para salvar.")
        return

    timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    filename = f"gravacao_{timestamp}.mp4"
    
    print(f"üé¨ Salvando v√≠deo: {filename}")
    
    # Usa o codec 'mp4v' para compatibilidade
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(filename, fourcc, fps, resolution)

    for frame in list(video_buffer):
        out.write(frame)

    out.release()
    print(f"‚úÖ V√≠deo salvo com sucesso!")


def check_arms_crossed_above_head(keypoints, frame_shape):
    """
    Verifica se a pose 'bra√ßos cruzados acima da cabe√ßa' √© detectada.
    `keypoints` √© um array com formato (17, 3) para [x, y, confidence].
    """
    if keypoints.shape[0] < 17:
        return False

    h, w = frame_shape

    # Extrai os pontos-chave necess√°rios
    nose_pt = keypoints[NOSE]
    left_shoulder_pt = keypoints[LEFT_SHOULDER]
    right_shoulder_pt = keypoints[RIGHT_SHOULDER]
    left_wrist_pt = keypoints[LEFT_WRIST]
    right_wrist_pt = keypoints[RIGHT_WRIST]

    # Verifica a confian√ßa (s√≥ analisa se os pontos-chave foram detectados com certeza)
    min_confidence = 0.5
    if any(pt[2] < min_confidence for pt in [nose_pt, left_shoulder_pt, right_shoulder_pt, left_wrist_pt, right_wrist_pt]):
        return False

    # Condi√ß√£o 1: Pulsos acima dos ombros e do nariz (coordenada Y menor)
    wrist_above_shoulders = left_wrist_pt[1] < left_shoulder_pt[1] and right_wrist_pt[1] < right_shoulder_pt[1]
    wrist_above_nose = left_wrist_pt[1] < nose_pt[1] and right_wrist_pt[1] < nose_pt[1]

    # Condi√ß√£o 2: Pulsos cruzados (pulso esquerdo mais √† direita que o direito)
    wrists_crossed = left_wrist_pt[0] > right_wrist_pt[0]

    return wrist_above_shoulders and wrist_above_nose and wrists_crossed


def main():
    # --- Inicializa√ß√£o ---
    picam2 = Picamera2()
    config = picam2.create_preview_configuration(main={"size": CAMERA_RESOLUTION, "format": "RGB888"})
    picam2.configure(config)
    picam2.start()
    print("üì∑ C√¢mera iniciada.")

    target, hef, input_vstream, output_vstream = setup_hailo_device(HEF_PATH)
    
    # Determina o FPS da c√¢mera e o tamanho do buffer
    # A captura e o processamento podem n√£o atingir o FPS m√°ximo, ent√£o medimos.
    # Por seguran√ßa, calculamos o buffer com um FPS estimado de 15.
    estimated_fps = 15
    buffer_size = VIDEO_DURATION_SECONDS * estimated_fps
    video_buffer = deque(maxlen=buffer_size)

    last_trigger_time = 0

    print("üöÄ Iniciando loop de detec√ß√£o...")
    try:
        while True:
            # Captura e pr√©-processamento do frame
            frame = picam2.capture_array()
            
            # Adiciona o frame ao buffer circular
            video_buffer.append(frame)

            # Envia o frame para infer√™ncia no Hailo-8L
            with input_vstream.write_async(frame):
                # Enquanto a infer√™ncia acontece, podemos fazer outras coisas se necess√°rio
                # Aqui, vamos esperar pelo resultado
                result_raw = output_vstream.read(timeout=1000) # Timeout de 1s
            
            # O p√≥s-processamento depende da sa√≠da exata do seu modelo YOLOv8.
            # Geralmente √© uma combina√ß√£o de caixas delimitadoras e pontos-chave.
            # Assumindo que a sa√≠da j√° foi processada para extrair os keypoints
            # de uma pessoa. Esta parte pode precisar de ajuste!
            
            # **ADAPTE ESTA PARTE CONFORME A SA√çDA DO SEU MODELO**
            # Exemplo: Vamos assumir que `result_raw` cont√©m um array de detec√ß√µes,
            # e cada detec√ß√£o tem a pose. Para simplificar, pegamos a primeira.
            # A forma da sa√≠da pode ser (1, 84, 8400) ou similar.
            # Voc√™ precisar√° de uma fun√ß√£o de p√≥s-processamento para decodificar isso.
            
            # Placeholder para a l√≥gica de p√≥s-processamento real
            # Aqui, simulamos que `post_process` retorna uma lista de poses detectadas
            # e cada pose √© um array (17, 3) de [x, y, conf].
            #detections = post_process_yolov8_pose(result_raw)
            
            # --- SIMULA√á√ÉO DE DETEC√á√ÉO PARA TESTE ---
            # Para testar sem a l√≥gica de p√≥s-processamento complexa, voc√™ pode
            # pular a infer√™ncia e desenhar pontos-chave falsos na tela.
            # Esta se√ß√£o deve ser substitu√≠da pela sua l√≥gica de p√≥s-processamento real.
            
            # --- L√ìGICA DE DETEC√á√ÉO REAL ---
            # (Aqui voc√™ implementaria a decodifica√ß√£o da sa√≠da `result_raw`)
            # Por enquanto, vamos assumir que n√£o h√° detec√ß√£o para o loop continuar.
            detections = [] 

            # Itera sobre todas as pessoas detectadas no frame
            for person_keypoints in detections:
                if time.time() - last_trigger_time < COOLDOWN_SECONDS:
                    continue # Respeita o tempo de espera

                is_triggered = check_arms_crossed_above_head(person_keypoints, frame.shape[:2])

                if is_triggered:
                    print(f"üö® Gesto detectado! Salvando {VIDEO_DURATION_SECONDS} segundos de v√≠deo.")
                    last_trigger_time = time.time()
                    
                    # Salva o v√≠deo em uma thread separada para n√£o bloquear a captura
                    save_thread = threading.Thread(target=save_video_from_buffer, args=(list(video_buffer), CAMERA_RESOLUTION, estimated_fps))
                    save_thread.start()
                    break # Sai do loop de detec√ß√µes para este frame
            
            # Opcional: Mostrar o v√≠deo na tela para depura√ß√£o
            # cv2.imshow("Camera Feed", frame)
            # if cv2.waitKey(1) & 0xFF == ord('q'):
            #     break

    except KeyboardInterrupt:
        print("\nüõë Parando o script.")
    finally:
        picam2.stop()
        # cv2.destroyAllWindows()
        print("Recursos liberados.")


if __name__ == "__main__":
    if not os.path.exists(HEF_PATH):
        print(f"ERRO: Arquivo {HEF_PATH} n√£o encontrado. Certifique-se de que ele est√° no mesmo diret√≥rio do script.")
    else:
        main()